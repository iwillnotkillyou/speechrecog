{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mNMl0C8vmI3"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "5sxUE4CdUM0H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e96ba07e-dc3e-4b11-bde5-559d531b4e28",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.025119Z",
     "start_time": "2025-07-30T22:55:29.940807Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget -nc -O data.zip \"https://www.dropbox.com/scl/fo/6rets9zuu0nvbbokxvf0e/AIa4cSpy_sjdd7Pz9aTxXa0?rlkey=vqj56pc1sp6qlggldrz5vdw32&st=6mf25abu&dl=0\"\n",
    "#!unzip -o data.zip\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "qRNDy707Q9aG",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.116796800Z",
     "start_time": "2025-07-30T22:55:29.964630500Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def fact_from_dict(fact_dict: Dict):\n",
    "    if fact_dict[\"fact_parent\"] is None:\n",
    "        return Fact(**fact_dict)\n",
    "    else:\n",
    "        fact_dict = copy.deepcopy(fact_dict)\n",
    "        fact_parent_entry = fact_dict.pop(\"fact_parent\")\n",
    "        fact_parent = Fact(**fact_parent_entry)\n",
    "        return Fact(**fact_dict, fact_parent=fact_parent)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fact:\n",
    "    subject: str\n",
    "    rel_lemma: str\n",
    "    object: str\n",
    "    rel_p_id: str\n",
    "    query: str\n",
    "    fact_paragraph: str = None\n",
    "    fact_parent: \"Fact\" = None\n",
    "    intermediate_paragraph: str = None\n",
    "\n",
    "    def get_subject(self) -> str:\n",
    "        return self.subject\n",
    "\n",
    "    def get_relation_property_id(self) -> str:\n",
    "        return self.rel_p_id\n",
    "\n",
    "    def get_object(self) -> str:\n",
    "        return self.object\n",
    "\n",
    "    def get_relation(self) -> str:\n",
    "        return self.rel_lemma\n",
    "\n",
    "    def get_paragraph(self) -> str:\n",
    "        return self.fact_paragraph\n",
    "\n",
    "    def get_intermediate_paragraph(self) -> str:\n",
    "        return self.intermediate_paragraph\n",
    "\n",
    "    def get_parent(self) -> \"Fact\":\n",
    "        return self.fact_parent\n",
    "\n",
    "    def get_query(self) -> str:\n",
    "        return self.query\n",
    "\n",
    "    def as_tuple(self):\n",
    "        return self.subject, self.rel_p_id, self.object\n",
    "\n",
    "    def as_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        if self.fact_parent is not None:\n",
    "            output[\"fact_parent\"] = output[\"fact_parent\"].as_dict()\n",
    "        return output\n",
    "\n",
    "    def __eq__(self, o: \"Fact\") -> bool:\n",
    "        return o.subject == self.subject and o.object == self.object and o.rel_p_id == self.rel_p_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "iuc8L9TGQr1D",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.130463700Z",
     "start_time": "2025-07-30T22:55:29.988195900Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from contextlib import AbstractContextManager\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ResumeAndSaveDataset(AbstractContextManager, ABC):\n",
    "    \"\"\"\n",
    "    A context manager that resumes processing a data from where it left off, saves the output periodically,\n",
    "    and ensures the output is saved in case of an exception.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, save_interval=1000):\n",
    "        \"\"\"\n",
    "        Initializes the ResumeAndSaveDataset context manager.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path of the data file.\n",
    "            save_interval (int, optional): The number of entries to process before saving the output data.\n",
    "                                           Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.output_dataset = self.load_output_dataset()\n",
    "        self.save_interval = save_interval\n",
    "        self.entries_since_last_save = 0\n",
    "\n",
    "    def load_output_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads the output data from the specified file. If the file is not found, an empty list is returned.\n",
    "\n",
    "        Returns:\n",
    "            List: The output data loaded from the file or an empty list if the file is not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            get_logger().info(f\"Loaded {len(data)} previously computed entries from the data stored in {self.path}.\")\n",
    "            return data\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_input_processed(self, inp: dict):\n",
    "        \"\"\"\n",
    "        Check if the input dictionary has been processed.\n",
    "\n",
    "        Args:\n",
    "            inp (dict): A dictionary containing the input data to be checked.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the input has been processed, False otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_entry(self, entry):\n",
    "        \"\"\"\n",
    "        Appends a new data entry to the output_dataset and saves the output data if the save_interval is reached.\n",
    "\n",
    "        Args:\n",
    "            entry: The data entry to be added.\n",
    "        \"\"\"\n",
    "        self.output_dataset.append(entry)\n",
    "        self.entries_since_last_save += 1\n",
    "\n",
    "        if self.entries_since_last_save >= self.save_interval:\n",
    "            self.save_output_dataset()\n",
    "            self.entries_since_last_save = 0\n",
    "\n",
    "    def save_output_dataset(self):\n",
    "        \"\"\"\n",
    "        Saves the output data to the file.\n",
    "        \"\"\"\n",
    "        with open(self.path, \"w+\") as f:\n",
    "            json.dump(self.output_dataset, f, indent=4)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        The enter method for the context manager.\n",
    "\n",
    "        Returns:\n",
    "            ResumeAndSaveDataset: The instance of the context manager.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"\n",
    "        The exit method for the context manager. Saves the output data to the file and prints a message.\n",
    "\n",
    "        Args:\n",
    "            exc_type: The type of the exception, if any.\n",
    "            exc_value: The instance of the exception, if any.\n",
    "            traceback: A traceback object, if any.\n",
    "\n",
    "        Returns:\n",
    "            bool: False to propagate the exception, True to suppress it.\n",
    "        \"\"\"\n",
    "        self.save_output_dataset()\n",
    "        if exc_type is not None:\n",
    "            get_logger().info(f\"Output data saved in the following location due to an exception: {self.path}\")\n",
    "        else:\n",
    "            get_logger().info(f\"Output data saved in the following location: {self.path}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class ResumeAndSaveFactDataset(ResumeAndSaveDataset):\n",
    "    def __init__(self, path, save_interval=20):\n",
    "        super().__init__(path, save_interval)\n",
    "        self.entry_processed = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: False))))\n",
    "\n",
    "        for entry in self.output_dataset:\n",
    "            entry_fact = fact_from_dict(entry[\"fact\"] if \"fact\" in entry else entry)\n",
    "            subject = entry_fact.get_subject()\n",
    "            rel = entry_fact.get_relation_property_id()\n",
    "            obj = entry_fact.get_object()\n",
    "            intermediate_paragraph = entry_fact.get_intermediate_paragraph()\n",
    "            self.entry_processed[subject][rel][obj][intermediate_paragraph] = True\n",
    "\n",
    "    def is_input_processed(self, fact: Fact):\n",
    "        return self.entry_processed[fact.get_subject()][fact.get_relation_property_id()][fact.get_object()][\n",
    "            fact.get_intermediate_paragraph()\n",
    "        ]\n",
    "\n",
    "    def add_entry(self, entry: Dict):\n",
    "        super().add_entry(entry)\n",
    "\n",
    "        if \"fact\" in entry:\n",
    "            fact = fact_from_dict(entry[\"fact\"])\n",
    "        else:\n",
    "            fact = fact_from_dict(entry)\n",
    "\n",
    "        self.entry_processed[fact.get_subject()][fact.get_relation_property_id()][fact.get_object()][\n",
    "            fact.get_intermediate_paragraph()\n",
    "        ] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMKfCY_ivYIW"
   },
   "source": [
    "# Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "lwjjvwTBQfR2",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.148869200Z",
     "start_time": "2025-07-30T22:55:30.020080200Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Union, List\n",
    "import torch\n",
    "from operator import attrgetter\n",
    "import subprocess\n",
    "import torchaudio\n",
    "from transformers.models.whisper.modeling_whisper import WhisperDecoder\n",
    "from transformers import AutoProcessor\n",
    "# Code partially adapted from https://github.com/kmeng01/rome\n",
    "\n",
    "def run_festival(text):\n",
    "    print(text)\n",
    "    with open(\"temp.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "    subprocess.run(\"text2wave temp.txt -o temp.wav\".split(\" \"))\n",
    "    waveform, sample_rate = torchaudio.load(\"temp.wav\")\n",
    "    return waveform[0].numpy(), sample_rate\n",
    "\n",
    "class ModelForwarder:\n",
    "    def __init__(self):\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
    "    \n",
    "    def forward(self, model, tokenizer, prompt, device, repeat, obj):\n",
    "        samplep = run_festival(prompt)\n",
    "        samples = run_festival(obj)\n",
    "        sample_rate = samplep[1]\n",
    "        sample = np.concatenate([samplep[0], samples[0]+np.random.normal(0, np.full_like(samples[0], 0.2))], 0)\n",
    "        input_features = self.processor([sample], sampling_rate=sample_rate, return_tensors=\"pt\",\n",
    "                           pad_to_multiple_of=8).input_features[-model.config.max_source_positions+5:].to(device).to(model.dtype)\n",
    "        decoder_input = torch.tensor([tokenizer.encode(prompt)[:-1]], device=device)[-model.config.max_target_positions+5:]\n",
    "        logits = model.forward(torch.cat([input_features]*2, 0) if repeat else input_features, decoder_input_ids=torch.cat([decoder_input]*2, 0) if repeat else decoder_input)\n",
    "        return logits\n",
    "\n",
    "def get_next_token(model, tokenizer, prompt, device, model_forwarder, repeat, obj):\n",
    "    # Prepare inputs\n",
    "\n",
    "    # Feed model\n",
    "    with torch.no_grad():\n",
    "        next_token_logits = model_forwarder.forward(model, tokenizer, prompt, device, repeat, obj)[\"logits\"].detach()[0, -1, :]\n",
    "\n",
    "    # Find the token with the highest probability and its logit\n",
    "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    max_prob, max_prob_index = torch.max(next_token_probs, dim=-1)\n",
    "    max_prob = max_prob.item()\n",
    "    max_prob_index = max_prob_index.item()\n",
    "\n",
    "    # Convert the token index back to a string\n",
    "    token = tokenizer.convert_ids_to_tokens(max_prob_index)\n",
    "\n",
    "    return token, max_prob\n",
    "\n",
    "\n",
    "def get_next_token_probabilities(\n",
    "    model, tokenizer, prompt: str, target_tokens: Union[str, List[str]], device, model_forwarder, repeat, obj\n",
    "):\n",
    "    # Make input as list of strings if a single string was given\n",
    "\n",
    "    if type(target_tokens) == str:\n",
    "        target_tokens = [target_tokens]\n",
    "\n",
    "    # Prepare inputs\n",
    "    target_token_ids = [tokenizer.convert_tokens_to_ids(next_token) for next_token in target_tokens]\n",
    "\n",
    "    # Feed model\n",
    "    with torch.no_grad():\n",
    "        next_token_logits = model_forwarder.forward(model, tokenizer, prompt, device, repeat, obj)[\"logits\"][:, -1, :]\n",
    "\n",
    "    # Extract target token logits and probabilities\n",
    "    target_token_probs = torch.softmax(next_token_logits, dim=-1)[:, target_token_ids]\n",
    "\n",
    "    return target_token_probs\n",
    "\n",
    "\n",
    "def adapt_target_tokens(tokenizer, target_tokens: List[str], preprend_space: bool):\n",
    "    \"\"\"\n",
    "    Make sure that target_tokens contain correspond to only a single token\n",
    "    \"\"\"\n",
    "    if preprend_space:\n",
    "        target_tokens = [\" \" + token.lstrip() for token in target_tokens]\n",
    "\n",
    "    target_tokens = [tokenizer.tokenize(token)[0] for token in target_tokens]\n",
    "\n",
    "    return target_tokens\n",
    "\n",
    "\n",
    "def find_substring_range(tokenizer, string, substring):\n",
    "    string_ids = tokenizer(\n",
    "        string,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=False,\n",
    "    )[\"input_ids\"]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(string_ids)\n",
    "    string = \"\".join(tokens)\n",
    "\n",
    "    substring_ids = tokenizer.tokenize(substring)\n",
    "    substring = \"\".join(substring_ids)\n",
    "\n",
    "    char_loc = string.rindex(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(tokens):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "\n",
    "    return tok_start, tok_end\n",
    "\n",
    "def get_module_name(model, kind, num=None):\n",
    "    if hasattr(model, \"transformer\"):\n",
    "        if kind == \"embed\":\n",
    "            return \"transformer.wte\"\n",
    "        return f'transformer.h.{num}{\"\" if kind == \"hidden\" else \".\" + kind}'\n",
    "    print(model.__dict__.keys())\n",
    "    print(type(model))\n",
    "    if isinstance(model, WhisperDecoder):\n",
    "        if kind == \"embed\":\n",
    "            return \"model.decoder.embed_tokens\"\n",
    "        if kind == \"attn\":\n",
    "            kind = \"self_attn\"\n",
    "        if kind == \"mlp\":\n",
    "            kind = \"fc2\"\n",
    "        return f'model.decoder.layers.{num}{\"\" if kind == \"hidden\" else \".\" + kind}'\n",
    "    if hasattr(model, \"model\"):\n",
    "        if kind == \"embed\":\n",
    "            return \"model.decoder.embed_tokens\"\n",
    "        if kind == \"attn\":\n",
    "            kind = \"self_attn\"\n",
    "        if kind == \"mlp\":\n",
    "            kind = \"fc2\"\n",
    "        return f'model.decoder.layers.{num}{\"\" if kind == \"hidden\" else \".\" + kind}'\n",
    "    assert False, \"unknown transformer structure\"\n",
    "\n",
    "\n",
    "def get_num_layers(model):\n",
    "    return len([n for n, m in model.named_modules() if (re.match(r\"^(transformer|model|model.decoder)\\.(h|layers)\\.\\d+$\", n))])\n",
    "\n",
    "\n",
    "def get_num_tokens(tokenizer, string):\n",
    "    tokens_ids = tokenizer(\n",
    "        string,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=False,\n",
    "    )[\"input_ids\"]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "    # print(\"tokens: \\n\", list(zip(range(len(tokens)), tokens)))\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def find_submodule(module, name):\n",
    "    \"\"\"\n",
    "    Finds the named module within the given model.\n",
    "    \"\"\"\n",
    "    for n, m in module.named_modules():\n",
    "        if n == name:\n",
    "            return m\n",
    "    raise LookupError(name)\n",
    "\n",
    "\n",
    "def get_embedding(model, token_id, device):\n",
    "    # Prepare inputs\n",
    "    token_ids = torch.tensor([[token_id]], device=device)\n",
    "\n",
    "    # Feed model\n",
    "    embed_module = model.model.decoder.embed_tokens\n",
    "    embedding = embed_module(token_ids)[0, 0, :]\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "0gU20YXQQYxy",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.148869200Z",
     "start_time": "2025-07-30T22:55:30.037354600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Code partially adapted from https://github.com/kmeng01/rome\n",
    "\n",
    "class MaskedCausalTracer:\n",
    "    def __init__(self, model: nn.Module, tokenizer: Tokenizer, mask_token: str, model_forwarder):\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token = mask_token\n",
    "        self.mask_token_embedding = self._get_mask_token_embedding(mask_token)\n",
    "        self.model_forwarder = model_forwarder\n",
    "\n",
    "    def _get_mask_token_embedding(self, mask_token):\n",
    "        token_attr = f\"{mask_token}_token_id\"\n",
    "        if getattr(self.tokenizer, token_attr, None) is not None:\n",
    "            mask_token_id = getattr(self.tokenizer, token_attr)\n",
    "        else:\n",
    "            raise ValueError(\"No such token in the tokenizer.\")\n",
    "        with torch.no_grad():\n",
    "            corrupted_token_embedding = get_embedding(self.model, mask_token_id, self.device).clone()\n",
    "        return corrupted_token_embedding\n",
    "\n",
    "    def trace_with_patch(\n",
    "        self,\n",
    "        prompt,\n",
    "        range_to_mask,  # A tuple (start, end) of tokens to corrupt\n",
    "        target_tokens,  # Tokens whose probabilities we are interested in\n",
    "        states_to_patch,  # A list of tuples (token index, modules) of states to restore\n",
    "        embedding_module_name,  # Name of the embedding layer\n",
    "        obj\n",
    "    ):\n",
    "        def untuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "        hooks = []\n",
    "\n",
    "        # Add embedding hook\n",
    "        def hook_embedding(module, input, output):\n",
    "            output[1, range_to_mask[0] : range_to_mask[1]] = self.mask_token_embedding.clone()\n",
    "            return output\n",
    "        print(type(self.model))\n",
    "        embedding_module = find_submodule(self.model, embedding_module_name)\n",
    "        embedding_hook = embedding_module.register_forward_hook(hook_embedding)\n",
    "        hooks.append(embedding_hook)\n",
    "\n",
    "        # Add hooks for the modules to restore\n",
    "        print([x for x,y in self.model.named_modules()])\n",
    "        for token_to_restore, modules_to_restore in states_to_patch:\n",
    "            for module_name in modules_to_restore:\n",
    "                def restoring_hook(module, input, output):\n",
    "                    h = untuple(output)\n",
    "                    h[1, token_to_restore] = h[0, token_to_restore].clone()\n",
    "                    return output\n",
    "\n",
    "                module = find_submodule(self.model, module_name)\n",
    "                module_hook = module.register_forward_hook(restoring_hook)\n",
    "                hooks.append(module_hook)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = get_next_token_probabilities(self.model, self.tokenizer, prompt, target_tokens, self.device, self.model_forwarder, True, obj)\n",
    "            clean_probs = probs[0, :]\n",
    "            corrupted_probs = probs[1, :]\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        return clean_probs, corrupted_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "1o8ZZiEmQY7K",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.148869200Z",
     "start_time": "2025-07-30T22:55:30.043551600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "# Code partially adapted from https://github.com/kmeng01/rome\n",
    "\n",
    "class Feature:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.ids = []\n",
    "        self.d = []\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def to_array(self):\n",
    "        return np.array(self.d)\n",
    "\n",
    "    def add(self, v):\n",
    "        self.d.append(v)\n",
    "        self.ids.append(len(self.d))\n",
    "\n",
    "    def avg(self):\n",
    "        np_array = np.array(self.d)\n",
    "        return np.mean(np_array[~np.isnan(np_array)])\n",
    "\n",
    "    def std(self):\n",
    "        np_array = np.array(self.d)\n",
    "        return np.std(np_array[~np.isnan(np_array)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.d)\n",
    "\n",
    "    def get_w_id(self, i):\n",
    "        return self.d[i], self.ids[i]\n",
    "    \n",
    "    def get(self, i):\n",
    "        return self.d[i]\n",
    "\n",
    "\n",
    "def group_results(facts, bucket):\n",
    "    labels = [\"subj-first\", \"subj-middle\", \"subj-last\", \"cont-first\", \"cont-middle\", \"cont-last\"]\n",
    "\n",
    "    corrupted_probs = Feature(\"corr\")\n",
    "    clean_probs = Feature(\"clean\")\n",
    "    results = {kind: {labels[i]: Feature(labels[i]) for i in range(len(labels))} for kind in [\"hidden\", \"mlp\", \"attn\"]}\n",
    "\n",
    "    target_token = f\"{bucket}_token\"\n",
    "\n",
    "    for processed_fact in facts:\n",
    "        processed_fact = processed_fact[\"results\"]\n",
    "        corrupted_score = processed_fact[\"corrupted\"][target_token][\"probs\"]\n",
    "        clean_score = processed_fact[\"clean\"][target_token][\"probs\"]\n",
    "\n",
    "        # If there is a zero interval, skip the fact\n",
    "        interval_to_explain = max(clean_score - corrupted_score, 0)\n",
    "        if interval_to_explain == 0:\n",
    "            continue\n",
    "\n",
    "        corrupted_probs.add(corrupted_score)\n",
    "        clean_probs.add(clean_score)\n",
    "\n",
    "        for kind in [\"hidden\", \"mlp\", \"attn\"]:\n",
    "            (\n",
    "                avg_first_subject,\n",
    "                avg_middle_subject,\n",
    "                avg_last_subject,\n",
    "                avg_first_after,\n",
    "                avg_middle_after,\n",
    "                avg_last_after,\n",
    "            ) = results[kind].values()\n",
    "\n",
    "            tokens = processed_fact[\"tokens\"]\n",
    "            started_subject = False\n",
    "            finished_subject = False\n",
    "            temp_mid = 0.0\n",
    "            count_mid = 0\n",
    "\n",
    "            for token in tokens:\n",
    "                interval_explained = max(token[kind][target_token][\"probs\"] - corrupted_score, 0)\n",
    "                token_effect = min(interval_explained / interval_to_explain, 1)\n",
    "\n",
    "                if \"subject_pos\" in token:\n",
    "                    if not started_subject:\n",
    "                        avg_first_subject.add(token_effect)\n",
    "                        started_subject = True\n",
    "\n",
    "                        if token[\"subject_pos\"] == -1:\n",
    "                            avg_last_subject.add(token_effect)\n",
    "                    else:\n",
    "                        subject_pos = token[\"subject_pos\"]\n",
    "                        if subject_pos == -1:\n",
    "                            avg_last_subject.add(token_effect)\n",
    "                        else:\n",
    "                            temp_mid += token_effect\n",
    "                            count_mid += 1\n",
    "                else:\n",
    "                    if not finished_subject:\n",
    "                        # Process all subject middle tokens\n",
    "                        if count_mid > 0:\n",
    "                            avg_middle_subject.add(temp_mid / count_mid)\n",
    "                            temp_mid = 0.0\n",
    "                            count_mid = 0\n",
    "                        else:\n",
    "                            avg_middle_subject.add(0.0)\n",
    "                        avg_first_after.add(token_effect)\n",
    "                        finished_subject = True\n",
    "\n",
    "                        if token[\"pos\"] == -1:\n",
    "                            avg_last_after.add(token_effect)\n",
    "                    else:\n",
    "                        token_pos = token[\"pos\"]\n",
    "                        if token_pos == -1:\n",
    "                            avg_last_after.add(token_effect)\n",
    "                        else:\n",
    "                            temp_mid += token_effect\n",
    "                            count_mid += 1\n",
    "\n",
    "            if count_mid > 0:\n",
    "                avg_middle_after.add(temp_mid / count_mid)\n",
    "            else:\n",
    "                avg_middle_after.add(0.0)\n",
    "\n",
    "    return results, corrupted_probs, clean_probs\n",
    "\n",
    "\n",
    "def find_all_substring_range(tokenizer, string, substring):\n",
    "  string_ids = tokenizer(\n",
    "      string,\n",
    "      return_tensors=None,\n",
    "      return_token_type_ids=False,\n",
    "  )[\"input_ids\"]\n",
    "  tokens = tokenizer.convert_ids_to_tokens(string_ids)\n",
    "  string = \"\".join(tokens)\n",
    "  tokens1 = tokens\n",
    "\n",
    "  substring_ids = tokenizer.tokenize(substring)\n",
    "  substring = \"\".join(substring_ids)\n",
    "  r = []\n",
    "  while len(string) > 0 and string.count(substring) > 0:\n",
    "    char_loc = string.rindex(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(tokens1):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "    r.append((tok_start, tok_end))\n",
    "    string = string[:char_loc]\n",
    "    tokens1 = tokens1[:tok_start]\n",
    "  return r, tokens\n",
    "\n",
    "def process_entry(causal_tracer: MaskedCausalTracer, prompt: str, subject: str, obj: str, target_token: str, bucket: str):\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    embedding_module_name = get_module_name(causal_tracer.model, \"embed\", 0)\n",
    "    subject_tokens_range = find_substring_range(causal_tracer.tokenizer, prompt, subject)\n",
    "\n",
    "    object_tokens_range, tokens = find_all_substring_range(causal_tracer.tokenizer, prompt, obj)\n",
    "    output[\"object_tokens_range\"] = object_tokens_range\n",
    "    output[\"subject_tokens_range\"] = subject_tokens_range\n",
    "\n",
    "    # Get corrupted run results\n",
    "    clean_probs, corrupted_probs = causal_tracer.trace_with_patch(\n",
    "        prompt, subject_tokens_range, [target_token], [(None, [])], embedding_module_name, obj\n",
    "    )\n",
    "    corrupted_output = {\"token\": target_token, \"probs\": corrupted_probs[0].item()}\n",
    "    clean_output = {\"token\": target_token, \"probs\": clean_probs[0].item()}\n",
    "\n",
    "    output[\"results\"] = {\n",
    "        \"corrupted\": {\n",
    "            f\"{bucket}_token\": corrupted_output,\n",
    "        },\n",
    "        \"clean\": {\n",
    "            f\"{bucket}_token\": clean_output,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Get patched runs results\n",
    "    num_tokens = get_num_tokens(causal_tracer.tokenizer, prompt) - 1 #TODO the -1 should not be necesary nd it does not belong here for models other than Whisper\n",
    "    output[\"results\"][\"tokens\"] = list()\n",
    "    # We start the loop from the first subject token as patching previous tokens has no effect\n",
    "    for token_i in (list(range(subject_tokens_range[0], num_tokens))):\n",
    "        d = {}\n",
    "        d[\"pos\"] = token_i - num_tokens\n",
    "        d[\"val\"] = tokens[token_i]\n",
    "\n",
    "        # If token is part of the subject, store its relative negative position\n",
    "        if subject_tokens_range[0] <= token_i < subject_tokens_range[1]:\n",
    "            d[\"subject_pos\"] = token_i - subject_tokens_range[1]\n",
    "        nl = get_num_layers(causal_tracer.model)\n",
    "        params = [(kind, last_layer)for kind in [\"hidden\", \"mlp\", \"attn\"] for\n",
    "                  last_layer in range(1, nl+1, 1)]\n",
    "        patches = [(0, len(tokens))]\n",
    "        params_all = [(kind, last_layer, patch) for kind, last_layer in params\n",
    "                      for patch in patches]\n",
    "        print(\"params_all\", len(params_all), num_tokens)\n",
    "        for kind, last_layer, patch in params_all:\n",
    "            states_to_patch = (\n",
    "                token_i,\n",
    "                [\n",
    "                    get_module_name(causal_tracer.model, kind, L)\n",
    "                    for L in range(\n",
    "                        0,\n",
    "                        last_layer,\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "            _, patched_probs = causal_tracer.trace_with_patch(\n",
    "                prompt, subject_tokens_range, [target_token], [states_to_patch], embedding_module_name, obj\n",
    "            )\n",
    "            patched_output = {\"token\": target_token, \"probs\": patched_probs[0].item()}\n",
    "            patched_results = {\n",
    "                f\"{bucket}_token\": patched_output,\n",
    "            }\n",
    "            if kind not in d:\n",
    "              d[kind] = {}\n",
    "            d[kind][last_layer] = patched_results\n",
    "        output[\"results\"][\"tokens\"].append(d)\n",
    "    return output\n",
    "\n",
    "\n",
    "def construct_prompt(fact: Fact, prompt_template):\n",
    "    prompt = prompt_template.format(query=fact.get_query(), context=fact.get_paragraph())\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def run_causal_tracing_analysis(\n",
    "    model: nn.Module,\n",
    "    tokenizer: Tokenizer,\n",
    "    fakepedia,\n",
    "    prompt_template,\n",
    "    num_grounded,\n",
    "    num_unfaithful,\n",
    "    prepend_space,\n",
    "    resume_dir,\n",
    "    model_forwarder,\n",
    "    skip_creation = True\n",
    "):\n",
    "    # We keep the results in two different files: unfaithful and grounded\n",
    "    #\n",
    "    # For each fact:\n",
    "    #\n",
    "    # Verify if the answer of the model is the unfaithful object or the grounded object. If the answer is another token, then skip the fact.\n",
    "    # Put the fact in the corresponding list.\n",
    "    #\n",
    "    # Once we have processed all the facts, for each list and for each fact of the list we run the causal tracer.\n",
    "    # Finally, we save the results in the corresponding file.\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    logger = get_logger()\n",
    "\n",
    "    if resume_dir is None:\n",
    "        resume_dir = get_output_dir()\n",
    "    os.makedirs(resume_dir, exist_ok=True)\n",
    "\n",
    "    partial_path = os.path.join(resume_dir, \"partial.json\")\n",
    "\n",
    "    if not skip_creation:\n",
    "        with ResumeAndSaveFactDataset(partial_path) as partial_dataset:\n",
    "            for entry in tqdm(fakepedia[:100], desc=\"Filtering facts\"):\n",
    "                fact = fact_from_dict(entry)\n",
    "                if partial_dataset.is_input_processed(fact):\n",
    "                    continue\n",
    "    \n",
    "                # Adapt unfaithful and grounded objects\n",
    "                target_tokens = adapt_target_tokens(\n",
    "                    tokenizer, [fact.get_parent().get_object(), fact.get_object()], prepend_space\n",
    "                )\n",
    "    \n",
    "                # Predict most likely next token\n",
    "                prompt = construct_prompt(fact, prompt_template)\n",
    "                most_likely_next_token, _ = get_next_token(model, tokenizer, prompt, device, model_forwarder, False, fact.get_object())\n",
    "    \n",
    "                partial_dataset.add_entry(\n",
    "                    {\n",
    "                        \"fact\": fact.as_dict(),\n",
    "                        \"partial_results\": {\n",
    "                            \"prompt\": prompt,\n",
    "                            \"next_token\": most_likely_next_token,\n",
    "                            \"unfaithful_token\": target_tokens[0],\n",
    "                            \"grounded_token\": target_tokens[1],\n",
    "                            \"is_unfaithful\": most_likely_next_token == target_tokens[0],\n",
    "                            \"is_grounded\": most_likely_next_token == target_tokens[1],\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    partial_dataset = read_json(partial_path)\n",
    "\n",
    "    unfaithful_facts = []\n",
    "    grounded_facts = []\n",
    "\n",
    "    for entry in partial_dataset:\n",
    "        if entry[\"partial_results\"][\"is_grounded\"]:\n",
    "            grounded_facts.append(entry)\n",
    "        elif entry[\"partial_results\"][\"is_unfaithful\"]:\n",
    "            unfaithful_facts.append(entry)\n",
    "\n",
    "    logger.info(f\"Found {len(unfaithful_facts)} unfaithful facts and {len(grounded_facts)} grounded facts\")\n",
    "\n",
    "    causal_tracer = MaskedCausalTracer(model, tokenizer, \"eos\", ModelForwarder())\n",
    "\n",
    "    for bucket in [\"grounded\", \"unfaithful\"]:\n",
    "        if bucket == \"unfaithful\":\n",
    "            if num_unfaithful == -1:\n",
    "                num_unfaithful = len(unfaithful_facts)\n",
    "            facts = unfaithful_facts[:num_unfaithful]\n",
    "        else:\n",
    "            if num_grounded == -1:\n",
    "                num_grounded = len(grounded_facts)\n",
    "            facts = grounded_facts[:num_grounded]\n",
    "\n",
    "        num_facts = len(facts)\n",
    "\n",
    "        causal_traces_path = os.path.join(resume_dir, f\"{bucket}.json\")\n",
    "\n",
    "        logger.info(f\"Running causal tracing on {num_facts} {bucket} facts\")\n",
    "        with ResumeAndSaveFactDataset(causal_traces_path, save_interval=1) as dataset:\n",
    "            for entry in tqdm(facts, desc=f\"Running causal tracing on {bucket} facts\"):\n",
    "\n",
    "                fact = fact_from_dict(entry[\"fact\"])\n",
    "\n",
    "                if dataset.is_input_processed(fact):\n",
    "                    continue\n",
    "\n",
    "                prompt = entry[\"partial_results\"][\"prompt\"]\n",
    "                target_token = entry[\"partial_results\"][\"next_token\"]\n",
    "\n",
    "                output_entry = process_entry(causal_tracer, prompt, fact.get_subject(), fact.get_object(), target_token, bucket)\n",
    "\n",
    "                output_entry[\"fact\"] = fact.as_dict()\n",
    "\n",
    "                dataset.add_entry(output_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7dtzf2XvOTv"
   },
   "source": [
    "# Logger and detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "sZyFrQj2P5nl",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.193668400Z",
     "start_time": "2025-07-30T22:55:30.094953500Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from logging import Logger\n",
    "from typing import Any\n",
    "\n",
    "import pytz\n",
    "\n",
    "import inspect\n",
    "\n",
    "\n",
    "def prepare_logger(output_dir: str) -> Logger:\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(filename)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # Log to file\n",
    "    fh = logging.FileHandler(os.path.join(output_dir, \"log.txt\"))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Log to console\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def prepare_output_dir(base_dir: str = \"./runs/\") -> str:\n",
    "    experiment_dir = os.path.join(\n",
    "        base_dir, datetime.now(tz=pytz.timezone(\"Europe/Zurich\")).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    )\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    return experiment_dir\n",
    "\n",
    "\n",
    "output_dir = prepare_output_dir()\n",
    "logger = prepare_logger(output_dir)\n",
    "\n",
    "\n",
    "def freeze_args(args: Any) -> None:\n",
    "    # Retrieve caller filename\n",
    "    caller_frame = inspect.stack()[1]\n",
    "    caller_filename_full = caller_frame.filename\n",
    "    caller_filename_only = os.path.splitext(os.path.basename(caller_filename_full))[0]\n",
    "\n",
    "    # Save args to json file\n",
    "    save_json(args.__dict__, os.path.join(output_dir, f\"{caller_filename_only}_args\"))\n",
    "\n",
    "\n",
    "def get_output_dir() -> str:\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def get_logger() -> Logger:\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Tl5QXq5eP6cO",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.193668400Z",
     "start_time": "2025-07-30T22:55:30.130463700Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def save_json(data: object, json_path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def read_json(json_path: str) -> Dict:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Dr3zAKwDSAG-",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.225440700Z",
     "start_time": "2025-07-30T22:55:30.167346100Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "def generate_datasets(\n",
    "    grounded_results,\n",
    "    unfaithful_results,\n",
    "    train_ratio=0.8,\n",
    "    n_samples_per_label=2000,\n",
    "    ablation_only_clean=False,\n",
    "    ablation_include_corrupted=False,\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "    logger = get_logger()\n",
    "    buckets = [grounded_results, unfaithful_results]\n",
    "\n",
    "    if ablation_only_clean:\n",
    "        feature_names = [grounded_results[2].get_name()]\n",
    "\n",
    "        if ablation_include_corrupted:\n",
    "            feature_names.append(grounded_results[1].get_name())\n",
    "\n",
    "    else:\n",
    "        feature_names = [\n",
    "            f\"{kind}-{feature}\" for kind, features in grounded_results[0].items() for feature in features.keys()\n",
    "        ]\n",
    "\n",
    "    logger.info(f\"Feature names: {feature_names}\")\n",
    "\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, bucket_results in enumerate(buckets):\n",
    "        kinds_results, corr_probs, clean_probs = bucket_results\n",
    "\n",
    "        num_samples = len(corr_probs)\n",
    "\n",
    "        logger.info(\"Number of samples: {}\".format(num_samples))\n",
    "\n",
    "        current_label_samples = []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            if ablation_only_clean:\n",
    "                candidate_example = [clean_probs.get(i)]\n",
    "\n",
    "                if ablation_include_corrupted:\n",
    "                    candidate_example.append(corr_probs.get(i))\n",
    "\n",
    "            else:\n",
    "                candidate_example = [\n",
    "                    feature_results.get(i)\n",
    "                    for kind_results in kinds_results.values()\n",
    "                    for feature_results in kind_results.values()\n",
    "                ]\n",
    "\n",
    "                if any([feature is None for feature in candidate_example]):\n",
    "                    continue\n",
    "\n",
    "            current_label_samples.append((candidate_example, label))\n",
    "        print(current_label_samples)\n",
    "        if len(current_label_samples) < n_samples_per_label:\n",
    "            raise ValueError(\n",
    "                f\"Bucket {label} has fewer than {n_samples_per_label} valid samples! In particular, there are {len(current_label_samples)} samples.\"\n",
    "            )\n",
    "\n",
    "        # Shuffle the samples for this label and take the first n_samples_per_label samples\n",
    "        np.random.shuffle(current_label_samples)\n",
    "        all_samples.extend([sample[0] for sample in current_label_samples[:n_samples_per_label]])\n",
    "        all_labels.extend([sample[1] for sample in current_label_samples[:n_samples_per_label]])\n",
    "\n",
    "    # Convert all_samples and all_labels to np arrays\n",
    "    all_samples_array = np.array(all_samples)\n",
    "    all_labels_array = np.array(all_labels)\n",
    "\n",
    "    # Calculate lengths for each split\n",
    "    total_size = len(all_samples_array)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "\n",
    "    # Shuffle and split the dataset\n",
    "    indices = np.arange(total_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_dataset = (all_samples_array[indices[:train_size]], all_labels_array[indices[:train_size]])\n",
    "    test_dataset = (all_samples_array[indices[train_size:]], all_labels_array[indices[train_size:]])\n",
    "    return train_dataset, test_dataset, feature_names\n",
    "\n",
    "\n",
    "def load_metrics(save_dir):\n",
    "    with open(os.path.join(save_dir, \"results.json\"), \"r\") as file:\n",
    "        results = json.load(file)\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_metrics(results, feature_names, save_dir):\n",
    "    with open(os.path.join(save_dir, \"results.json\"), \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "    if \"feature_importances\" in results:\n",
    "        importances = results[\"feature_importances\"]\n",
    "        indices = np.argsort(importances)\n",
    "\n",
    "        # Logic to determine the kind for colors\n",
    "        colors = {\"hidden\": \"grey\", \"mlp\": \"blue\", \"attn\": \"orange\", \"corr\": \"grey\", \"clean\": \"grey\"}\n",
    "\n",
    "        def determine_kind(verbose_name):\n",
    "            for kind, color in colors.items():\n",
    "                if kind in verbose_name.lower():\n",
    "                    return color\n",
    "            print(f\"Unmatched feature: {verbose_name}\")\n",
    "            raise ValueError(\"Unknown feature kind.\")\n",
    "\n",
    "        bar_colors = [determine_kind(name) for name in feature_names]\n",
    "\n",
    "        # Make the font size larger\n",
    "        plt.rcParams.update({\"font.size\": 21})\n",
    "\n",
    "        # Change the font family\n",
    "        plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.barh(\n",
    "            range(len(indices)),\n",
    "            [importances[i] for i in indices],\n",
    "            align=\"center\",\n",
    "            color=[bar_colors[i] for i in indices],\n",
    "            edgecolor=\"white\",\n",
    "        )\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel(\"Relative Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"feature_importances.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def save_decision_tree_plot(tree, feature_names, class_names, save_dir):\n",
    "    plt.figure(figsize=(200, 100))\n",
    "    plot_tree(tree, feature_names=feature_names, class_names=class_names, filled=True)\n",
    "    plt.savefig(os.path.join(save_dir, \"decision_tree.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(metrics_by_model, save_dir):\n",
    "    \"\"\"\n",
    "    metrics_by_model: dict, keys are model names (like 'Logistic Regression', 'DecisionTree', 'XGBoost') and values are\n",
    "                      dictionaries of metrics (keys are metric names, values are metric values)\n",
    "    save_dir: directory where plots will be saved\n",
    "    \"\"\"\n",
    "    model_colors = {\"LogisticRegression\": \"grey\", \"DecisionTree\": \"orange\", \"XGBoost\": \"blue\"}\n",
    "\n",
    "    # Validate that all models in metrics_by_model are known\n",
    "    for model in metrics_by_model:\n",
    "        if model not in model_colors:\n",
    "            raise Exception(f\"Unknown model: {model}\")\n",
    "\n",
    "    n_models = len(metrics_by_model)\n",
    "    n_metrics = len(metrics_by_model[next(iter(metrics_by_model))])\n",
    "\n",
    "    # Set bar width, distance between bars in a group, and positions\n",
    "    bar_width = 0.2\n",
    "    distance = 0.05  # distance between bars in a group\n",
    "    r1 = np.arange(n_metrics)  # positions for first model\n",
    "    r2 = [x + bar_width + distance for x in r1]  # positions for second model\n",
    "    r3 = [x + bar_width + distance for x in r2]  # positions for third model\n",
    "\n",
    "    # Make the font size larger\n",
    "    plt.rcParams.update({\"font.size\": 21})\n",
    "\n",
    "    # Change the font family\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plotting bars for each model\n",
    "    all_metric_values = []\n",
    "    for idx, (model, metrics) in enumerate(metrics_by_model.items()):\n",
    "        metric_values = [metrics[metric] for metric in metrics]\n",
    "        all_metric_values.extend(metric_values)\n",
    "        positions = [r1, r2, r3][idx]\n",
    "        plt.bar(positions, metric_values, color=model_colors[model], width=bar_width, edgecolor=\"white\", label=model)\n",
    "\n",
    "    # Adjust y-axis limit\n",
    "    plt.ylim(bottom=min(all_metric_values) * 0.9)\n",
    "\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    xtick_positions = [r2[i] for i in range(n_metrics)]  # Averages of r1 and r2 positions\n",
    "    plt.xticks(xtick_positions, list(metrics_by_model[next(iter(metrics_by_model))]))\n",
    "\n",
    "    # Place the legend outside the plot on the right\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, \"all_metrics_comparison.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_and_save(models, train_data, test_data, feature_names, class_names, seed, replot_only=False):\n",
    "    save_dir = get_output_dir()\n",
    "    plt.rcParams[\"font.size\"] = max(1, plt.rcParams[\"font.size\"])\n",
    "\n",
    "    metrics_by_model = {}\n",
    "\n",
    "    for model_name, model_info in models.items():\n",
    "        model_save_dir = os.path.join(save_dir, model_name)\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "        if not replot_only:\n",
    "            X_train, y_train = train_data\n",
    "            X_test, y_test = test_data\n",
    "\n",
    "            if \"random_state\" in model_info[\"model\"].get_params():\n",
    "                model_info[\"model\"].set_params(random_state=seed)\n",
    "\n",
    "            clf = GridSearchCV(model_info[\"model\"], model_info[\"param_grid\"], cv=5, verbose=10)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            y_train_proba = clf.predict_proba(X_train)[:, 1]\n",
    "            with open(f\"{model_name}_confusion_matrix.json\", \"w\") as f:\n",
    "                json.dump(confusion_matrix(y_test, y_pred).tolist(), f)\n",
    "            results = {\n",
    "                \"train\": {\n",
    "                    \"accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "                    \"precision\": precision_score(y_train, y_train_pred, average='weighted'),\n",
    "                    \"recall\": recall_score(y_train, y_train_pred, average='weighted'),\n",
    "                    \"f1_score\": f1_score(y_train, y_train_pred, average='weighted'),\n",
    "                    #\"roc_auc\": roc_auc_score(y_train, y_train_proba,\n",
    "                    #                         multi_class = \"ovr\", average='weighted'),\n",
    "                },\n",
    "                \"test\": {\n",
    "                    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                    \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "                    \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "                    \"f1_score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "                    #\"roc_auc\": roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1],\n",
    "                    #                         multi_class = \"ovr\", average='weighted'),\n",
    "                },\n",
    "                \"best_hyperparameters\": clf.best_params_,\n",
    "            }\n",
    "\n",
    "            if hasattr(clf.best_estimator_, \"feature_importances_\"):\n",
    "                # If there is an importance type attribute, print it\n",
    "                if hasattr(clf.best_estimator_, \"importance_type\"):\n",
    "                    print(f\"Feature importances: {clf.best_estimator_.importance_type}\")\n",
    "                results[\"feature_importances\"] = list(clf.best_estimator_.feature_importances_)\n",
    "                results[\"feature_importances\"] = [float(val) for val in results[\"feature_importances\"]]\n",
    "\n",
    "            if isinstance(clf.best_estimator_, LogisticRegression):\n",
    "                # Taking the absolute values of the coefficients\n",
    "                results[\"feature_importances\"] = [float(abs(val)) for val in clf.best_estimator_.coef_.flatten()]\n",
    "\n",
    "            save_metrics(results, feature_names, model_save_dir)\n",
    "\n",
    "            if model_name == \"DecisionTree\":\n",
    "                save_decision_tree_plot(clf.best_estimator_, feature_names, class_names, model_save_dir)\n",
    "        else:\n",
    "            results = load_metrics(model_save_dir)\n",
    "            save_metrics(results, feature_names, model_save_dir)\n",
    "\n",
    "        metrics_by_model[model_name] = results[\"test\"]\n",
    "\n",
    "    plot_metrics_comparison(metrics_by_model, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgRaaVLBvSSW"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "74mdFLEdP1Tj",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.225440700Z",
     "start_time": "2025-07-30T22:55:30.177655100Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "locs = [\"openai/whisper-base.en\"]\n",
    "\n",
    "class Namespace1:\n",
    "    def __init__(self):\n",
    "      self.token = None\n",
    "      self.fakepedia_path = \"base_fakepedia.json\"\n",
    "      self.model_name_path = locs[-1]\n",
    "      self.prompt_template = \"{context} {query}\"\n",
    "      self.num_grounded = 10\n",
    "      self.num_unfaithful = 10\n",
    "      self.prepend_space = True\n",
    "      self.bfloat16 = True\n",
    "      self.resume_dir = \"runs/2025-07-27_14-57-18\"\n",
    "      self.subset_size = 2\n",
    "      self.skip_creation = True\n",
    "\n",
    "class mock_model:\n",
    "    def __init__(self):\n",
    "        self.config = SimpleNamespace()\n",
    "        self.config.pad_token_id = 0\n",
    "        self.config.eos_token_id = 0\n",
    "        self.config.vocab_size = 1000\n",
    "        self.device_map = \"cpu\"\n",
    "\n",
    "def make_model(args, mock=False):\n",
    "    from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, BitsAndBytesConfig\n",
    "    cuda = torch.cuda.is_available()\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16) if cuda else None\n",
    "    model = mock_model() if mock else AutoModelForSpeechSeq2Seq.from_pretrained(args.model_name_path, token=args.token,\n",
    "                                                                           force_download=False,\n",
    "                                                                           quantization_config=quantization_config,\n",
    "                                                                           device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_path, token=args.token, force_download=False,\n",
    "                                              add_bos_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "def run_causal_tracing(args):\n",
    "    logger = get_logger()\n",
    "\n",
    "    logger.info(\"Loading fakepedia...\")\n",
    "    fakepedia = read_json(args.fakepedia_path)\n",
    "    #23 kinds of relations 1673 unique templates.\n",
    "    print(len(set([x[\"subject\"] for x in fakepedia])),\n",
    "          len(set([x[\"rel_p_id\"] for x in fakepedia])))\n",
    "\n",
    "    logger.info(\"Loading model...\")\n",
    "    model,tokenizer = make_model(args)\n",
    "\n",
    "    logger.info(\"Starting causal tracing...\")\n",
    "    run_causal_tracing_analysis(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        fakepedia,\n",
    "        args.prompt_template,\n",
    "        args.num_grounded,\n",
    "        args.num_unfaithful,\n",
    "        args.prepend_space,\n",
    "        args.resume_dir,\n",
    "        ModelForwarder(),\n",
    "        args.skip_creation\n",
    "    )\n",
    "\n",
    "\n",
    "def main1():\n",
    "    args = Namespace1()\n",
    "    freeze_args(args)\n",
    "    run_causal_tracing(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "BV7I8u2ER0oh",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:30.225440700Z",
     "start_time": "2025-07-30T22:55:30.210388300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "def set_seed_everywhere(seed: int) -> None:\n",
    "    # Set torch and numpy and random seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class Namespace2:\n",
    "    def __init__(self):\n",
    "      self.causal_traces_dir = \"./\"\n",
    "      self.dataset_name = \"simple\"\n",
    "      self.model_name = \"LLama\"\n",
    "      self.output_dir = \"out\"\n",
    "      self.balance = False\n",
    "      if False:\n",
    "          self.features_to_include = [\"subj-first\", \"subj-middle\", \"subj-last\", \"cont-first\", \"cont-middle\", \"cont-last\"]\n",
    "      else:\n",
    "          self.features_to_include = [\"subj-middle\", \"subj-last\", \"cont-middle\", \"cont-last\"]\n",
    "      self.kinds_to_include = [\"hidden\", \"mlp\"]\n",
    "      self.train_ratio = 0.5\n",
    "      self.ablation_only_clean = False\n",
    "      self.ablation_include_corrupted = False\n",
    "      self.seed = 2\n",
    "      self.num_classes = 10\n",
    "      self.n_samples_per_label = 2\n",
    "      self.min_count = 2\n",
    "\n",
    "def get_args():\n",
    "    return Namespace2()\n",
    "\n",
    "def process_facts2(target_token, facts, class_map, results, corrupted_probs, clean_probs, tokenizer):\n",
    "    for processed_fact in facts:\n",
    "        print(processed_fact[\"fact\"])\n",
    "        #print([(x, tokens[x[0]], f[\"object\"]) for x in object_tokens_range])\n",
    "        print(\"object_tokens_range\", object_tokens_range[0][0])\n",
    "        clas = class_map(processed_fact)\n",
    "        corrupted_score = processed_fact[\"results\"][\"corrupted\"][target_token][\"probs\"]\n",
    "        clean_score = processed_fact[\"results\"][\"clean\"][target_token][\"probs\"]\n",
    "\n",
    "        # If there is a zero interval, skip the fact\n",
    "        interval_to_explain = max(clean_score - corrupted_score, 0)\n",
    "        if interval_to_explain == 0:\n",
    "            continue\n",
    "\n",
    "        corrupted_probs[clas].add(corrupted_score)\n",
    "        clean_probs[clas].add(clean_score)\n",
    "\n",
    "        for kind in [\"hidden\", \"mlp\", \"attn\"]:\n",
    "            (\n",
    "                avg_first_subject,\n",
    "                avg_middle_subject,\n",
    "                avg_last_subject,\n",
    "                avg_first_after,\n",
    "                avg_middle_after,\n",
    "                avg_last_after,\n",
    "            ) = results[clas][kind].values()\n",
    "\n",
    "            tokens = processed_fact[\"results\"][\"tokens\"]\n",
    "            started_subject = False\n",
    "            finished_subject = False\n",
    "            temp_mid = 0.0\n",
    "            count_mid = 0\n",
    "\n",
    "            for token in tokens:\n",
    "                interval_explained_average = 0\n",
    "                print(\"token[kind].keys()\", token[kind].keys())\n",
    "                for layer in token[kind]:\n",
    "                    interval_explained_average += max(token[kind][layer][target_token][\"probs\"] - corrupted_score, 0) / len(token[kind])\n",
    "                token_effect = min(interval_explained_average / interval_to_explain, 1)\n",
    "\n",
    "                if \"subject_pos\" in token:\n",
    "                    if not started_subject:\n",
    "                        avg_first_subject.add(token_effect)\n",
    "                        started_subject = True\n",
    "\n",
    "                        if token[\"subject_pos\"] == -1:\n",
    "                            avg_last_subject.add(token_effect)\n",
    "                    else:\n",
    "                        subject_pos = token[\"subject_pos\"]\n",
    "                        if subject_pos == -1:\n",
    "                            avg_last_subject.add(token_effect)\n",
    "                        else:\n",
    "                            temp_mid += token_effect\n",
    "                            count_mid += 1\n",
    "                else:\n",
    "                    if not finished_subject:\n",
    "                        # Process all subject middle tokens\n",
    "                        if count_mid > 0:\n",
    "                            avg_middle_subject.add(temp_mid / count_mid)\n",
    "                            temp_mid = 0.0\n",
    "                            count_mid = 0\n",
    "                        else:\n",
    "                            avg_middle_subject.add(0.0)\n",
    "                        avg_first_after.add(token_effect)\n",
    "                        finished_subject = True\n",
    "\n",
    "                        if token[\"pos\"] == -1:\n",
    "                            avg_last_after.add(token_effect)\n",
    "                    else:\n",
    "                        token_pos = token[\"pos\"]\n",
    "                        if token_pos == -1:\n",
    "                            avg_last_after.add(token_effect)\n",
    "                        else:\n",
    "                            temp_mid += token_effect\n",
    "                            count_mid += 1\n",
    "\n",
    "            if count_mid > 0:\n",
    "                avg_middle_after.add(temp_mid / count_mid)\n",
    "            else:\n",
    "                avg_middle_after.add(0.0)\n",
    "\n",
    "def group_results2(facts_grounded, facts_unfaithful, tokenizer, args):\n",
    "    labels = [\"subj-first\", \"subj-middle\", \"subj-last\", \"cont-first\", \"cont-middle\", \"cont-last\"]\n",
    "    num_classes = args.num_classes\n",
    "    corrupted_probs = [Feature(\"corr\") for _ in range(num_classes)]\n",
    "    clean_probs = [Feature(\"clean\") for _ in range(num_classes)]\n",
    "    #Here results is a list of dictionaries, each dictionary contains the results for one class (i.e. label i.e. bucket), the bucket is decided in the process_facts2 function\n",
    "    results = [\n",
    "        {kind: {labels[i]: Feature(labels[i]) for i in range(6)} for kind in [\"hidden\", \"mlp\", \"attn\"]}\n",
    "    for _ in range(num_classes)]\n",
    "    if args.balance:\n",
    "        #Some rel_lemma are not present in all sets.\n",
    "        lemmauf = set(x[\"fact\"][\"rel_lemma\"] for x in facts_unfaithful)\n",
    "        lemmaf = set(x[\"fact\"][\"rel_lemma\"] for x in facts_grounded)\n",
    "        print(\"in facts_grounded not in facts_unfaithful\", lemmaf.difference(lemmauf))\n",
    "        print(\"in facts_unfaithful not in facts_grounded\", lemmauf.difference(lemmaf))\n",
    "        tempsuf = set(x[\"fact\"][\"subject\"] for x in facts_unfaithful)\n",
    "        tempsf = set(x[\"fact\"][\"subject\"] for x in facts_grounded)\n",
    "        #Templates not uniformly distributed, half of in unfaithful never appear in grounded\n",
    "        print(\"num unique templates facts_unfaithful\",\n",
    "              len(tempsuf))\n",
    "        print(\"num unique templates facts_grounded\",\n",
    "              len(tempsf))\n",
    "        print(\"num templates in facts_grounded not in facts_unfaithful\",\n",
    "              len(tempsf.difference(tempsuf)))\n",
    "        print(\"num templates in facts_unfaithful not in facts_grounded\",\n",
    "              len(tempsuf.difference(tempsf)))\n",
    "        #Remove trivial samples\n",
    "        trivial = tempsf.symmetric_difference(tempsuf)\n",
    "        facts_unfaithful = [x for x in facts_unfaithful if x[\"fact\"][\"subject\"] not in trivial]\n",
    "        facts_grounded = [x for x in facts_grounded if x[\"fact\"][\"subject\"] not in trivial]\n",
    "\n",
    "    process_facts2(\"unfaithful_token\", facts_unfaithful,\n",
    "                  lambda x: 0, num_classes, results, corrupted_probs, clean_probs, tokenizer)\n",
    "    facts_grounded\n",
    "    \n",
    "    process_facts2(\"grounded_token\", facts_grounded,\n",
    "                  , num_classes, results, corrupted_probs, clean_probs, tokenizer)\n",
    "    #print(\"corrupted_probs, clean_probs\", [x.d for x in corrupted_probs], [x.d for x in clean_probs])\n",
    "    vs = list((x, i, len(x[0][\"hidden\"][\"subj-first\"])) for i, x in enumerate(zip(results, corrupted_probs, clean_probs)))\n",
    "    #print([x[2] for x in vs])\n",
    "    vs = [(x, i) for x, i, l in vs if l >= args.min_count]\n",
    "    #in next experiment try [\"grounded\", \"confidently grounded\"]\n",
    "    return [x for x, i in vs], [f\"p:{i}\" for x, i in vs]\n",
    "\n",
    "def generate_datasets2(buckets,\n",
    "    train_ratio=0.8,\n",
    "    n_samples_per_label=2000\n",
    ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], Any]:\n",
    "    logger = get_logger()\n",
    "    feature_names = [\n",
    "            f\"{kind}-{feature}\" for kind, features in buckets[0][0].items() for feature in features.keys()\n",
    "        ]\n",
    "\n",
    "    logger.info(f\"Feature names: {feature_names}\")\n",
    "\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, bucket_results in enumerate(buckets):\n",
    "        kinds_results, corr_probs, clean_probs = bucket_results\n",
    "        #This is the correct number of samples the feature adding is done in a wierd way but there is always\n",
    "        #exactly one value in each feature for each sample\n",
    "        num_samples = len(corr_probs)\n",
    "\n",
    "        logger.info(\"Number of samples: {}\".format(num_samples))\n",
    "        print(label, [[(kind_name, feature_name, len(feature_results),\n",
    "                      feature_results.avg(), feature_results.std())\n",
    "                for feature_name, feature_results in kind_results.items()] for kind_name, kind_results in kinds_results.items()])\n",
    "\n",
    "        current_label_samples = []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            candidate_example = [\n",
    "                feature_results.get(i)\n",
    "                for kind_results in kinds_results.values()\n",
    "                for feature_results in kind_results.values()\n",
    "            ]\n",
    "            print(candidate_example, i, label)\n",
    "\n",
    "            if any([feature is None for feature in candidate_example]):\n",
    "                continue\n",
    "\n",
    "            current_label_samples.append((candidate_example, label))\n",
    "\n",
    "        if False and len(current_label_samples) < n_samples_per_label:\n",
    "            raise ValueError(\n",
    "                f\"Bucket {label} has fewer than {n_samples_per_label} valid samples! In particular, there are {len(current_label_samples)} samples.\"\n",
    "            )\n",
    "\n",
    "        # Shuffle the samples for this label and take the first n_samples_per_label samples\n",
    "        np.random.shuffle(current_label_samples)\n",
    "        all_samples.extend([sample[0] for sample in current_label_samples[:n_samples_per_label]])\n",
    "        all_labels.extend([sample[1] for sample in current_label_samples[:n_samples_per_label]])\n",
    "\n",
    "    # Convert all_samples and all_labels to np arrays\n",
    "    all_samples_array = np.array(all_samples)\n",
    "    all_labels_array = np.array(all_labels)\n",
    "\n",
    "    # Calculate lengths for each split\n",
    "    total_size = len(all_samples_array)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "\n",
    "    # Shuffle and split the dataset\n",
    "    indices = np.arange(total_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_dataset = (all_samples_array[indices[:train_size]], all_labels_array[indices[:train_size]])\n",
    "    test_dataset = (all_samples_array[indices[train_size:]], all_labels_array[indices[train_size:]])\n",
    "    print(test_dataset[0].shape, test_dataset[0].shape, feature_names)\n",
    "    return train_dataset, test_dataset, feature_names\n",
    "\"\"\"\n",
    "\"param_grid\": {\n",
    "                \"max_depth\": [None, 5, 10, 15, 20],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "            },\n",
    "\"param_grid\": {\n",
    "                \"max_depth\": [3, 4],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "            },\n",
    "\"\"\"\n",
    "def train_detector(args):\n",
    "    models = {\n",
    "        \"LogisticRegression\": {\n",
    "            \"model\": LogisticRegression(max_iter=1000),\n",
    "            \"param_grid\": {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"param_grid\": {\n",
    "                  \"max_depth\": [5],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "                \"ccp_alpha\": [0, 0.01, 0.1, 0.5, 0.9]\n",
    "            },\n",
    "            \"model\": DecisionTreeClassifier(),\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    buckets = [\"grounded\", \"unfaithful\"]\n",
    "    buckets_paths = [\n",
    "        os.path.join(args.causal_traces_dir, args.dataset_name, args.model_name, f\"{bucket}.json\") for bucket in buckets\n",
    "    ]\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-base.en\", force_download=False,\n",
    "                                              add_bos_token=True)\n",
    "\n",
    "    results, buckets = group_results2(read_json(buckets_paths[0]),\n",
    "                             read_json(buckets_paths[1]),\n",
    "                             tokenizer, args)\n",
    "    \n",
    "    # If we are only including certain kinds, filter the kinds\n",
    "    if args.kinds_to_include is not None:\n",
    "        results = [\n",
    "            (\n",
    "                {kind: bucket_results[0][kind] for kind in bucket_results[0] if kind in args.kinds_to_include},\n",
    "                bucket_results[1],\n",
    "                bucket_results[2],\n",
    "            )\n",
    "            for bucket_results in results\n",
    "        ]\n",
    "\n",
    "    # If we are only including certain features, filter the features\n",
    "    if args.features_to_include is not None:\n",
    "        results = [\n",
    "            (\n",
    "                {\n",
    "                    kind: {\n",
    "                        feature: bucket_results[0][kind][feature]\n",
    "                        for feature in bucket_results[0][kind]\n",
    "                        if feature in args.features_to_include\n",
    "                    }\n",
    "                    for kind in bucket_results[0]\n",
    "                },\n",
    "                bucket_results[1],\n",
    "                bucket_results[2],\n",
    "            )\n",
    "            for bucket_results in results\n",
    "        ]\n",
    "    print([[[len(z) for z in y] for y in x[0].values()] for x in results])\n",
    "    # Generate the datasets\n",
    "    train_data, test_data, feature_names = generate_datasets2(\n",
    "        results,\n",
    "        n_samples_per_label=args.n_samples_per_label,\n",
    "        train_ratio=args.train_ratio\n",
    "    )\n",
    "    print(len(train_data), len(train_data[0]), train_data[1])\n",
    "    print(len(test_data), len(test_data[0]), test_data[1])\n",
    "\n",
    "    # Train the models and save the results\n",
    "    train_and_save(models, train_data, test_data, feature_names, class_names=buckets, seed=args.seed)\n",
    "\n",
    "\n",
    "def main2():\n",
    "    sp = \"./run1\"\n",
    "    p = './simple/LLama'\n",
    "    os.makedirs(p, exist_ok = True)\n",
    "    gn = 'grounded.json'\n",
    "    un = 'unfaithful.json'\n",
    "    shutil.copyfile(os.path.join(sp,gn), os.path.join(p,gn))\n",
    "    shutil.copyfile(os.path.join(sp,un), os.path.join(p,un))\n",
    "    args = get_args()\n",
    "    freeze_args(args)\n",
    "    set_seed_everywhere(args.seed)\n",
    "    train_detector(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,843 - 1036489786.py - INFO - Feature names: ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,859 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n",
      "2025-07-31 00:55:31,898 - 1036489786.py - INFO - Number of samples: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "token[kind].keys() dict_keys(['1', '2', '3', '4', '5', '6'])\n",
      "[[[11, 9, 11, 9], [11, 9, 11, 9]], [[11, 9, 11, 9], [11, 9, 11, 9]]]\n",
      "0 [[('hidden', 'subj-middle', 2, np.float64(0.4195337891215452), np.float64(0.19267622308846694)), ('hidden', 'subj-last', 2, np.float64(0.05588562675701653), np.float64(0.043431414302804075)), ('hidden', 'cont-middle', 2, np.float64(0.005327902575495762), np.float64(0.005327902575495762)), ('hidden', 'cont-last', 2, np.float64(0.36028341635599104), np.float64(0.023531279603854316))], [('mlp', 'subj-middle', 2, np.float64(0.14710359581996216), np.float64(0.14710359581996216)), ('mlp', 'subj-last', 2, np.float64(0.047087138978030116), np.float64(0.047087138978030116)), ('mlp', 'cont-middle', 2, np.float64(0.12274863017889431), np.float64(0.0020376946074304736)), ('mlp', 'cont-last', 2, np.float64(0.466613724463638), np.float64(0.002983345133431542))]]\n",
      "[0.6122100122100121, 0.012454212454212455, 0.0, 0.3367521367521367, 0.0, 0.0, 0.12478632478632479, 0.4695970695970696] 0 0\n",
      "[0.22685756603307825, 0.0993170410598206, 0.010655805150991524, 0.38381469595984535, 0.2942071916399243, 0.09417427795606023, 0.12071093557146384, 0.4636303793302065] 1 0\n",
      "1 [[('hidden', 'subj-middle', 2, np.float64(0.0), np.float64(0.0)), ('hidden', 'subj-last', 2, np.float64(0.6669428746643589), np.float64(0.1965987378681937)), ('hidden', 'cont-middle', 2, np.float64(0.1788807658429588), np.float64(0.09918957674658622)), ('hidden', 'cont-last', 2, np.float64(0.37804402873919063), np.float64(0.0019194827439602546))], [('mlp', 'subj-middle', 2, np.float64(0.0), np.float64(0.0)), ('mlp', 'subj-last', 2, np.float64(0.004944249080441851), np.float64(0.004944249080441851)), ('mlp', 'cont-middle', 2, np.float64(0.010397461818006016), np.float64(0.00599290433070545)), ('mlp', 'cont-last', 2, np.float64(0.01389299691560248), np.float64(0.013527050304831091))]]\n",
      "[0.0, 0.47034413679616516, 0.0796911890963726, 0.3799635114831509, 0.0, 0.0, 0.004404557487300565, 0.02742004722043357] 0 1\n",
      "[0.0, 0.8635416125325526, 0.27807034258954505, 0.3761245459952304, 0.0, 0.009888498160883701, 0.016390366148711467, 0.0003659466107713895] 1 1\n",
      "(2, 8) (2, 8) ['hidden-subj-middle', 'hidden-subj-last', 'hidden-cont-middle', 'hidden-cont-last', 'mlp-subj-middle', 'mlp-subj-last', 'mlp-cont-middle', 'mlp-cont-last']\n",
      "2 2 [1 1]\n",
      "2 2 [0 0]\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[95]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m   main1()\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m   \u001B[43mmain2\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m   get_ipython().system(\u001B[33m'\u001B[39m\u001B[33mtail /content/runs/2025-03-19_17-42-58/log.txt\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      7\u001B[39m   \u001B[38;5;66;03m#!rm -r LLama\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 335\u001B[39m, in \u001B[36mmain2\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    333\u001B[39m freeze_args(args)\n\u001B[32m    334\u001B[39m set_seed_everywhere(args.seed)\n\u001B[32m--> \u001B[39m\u001B[32m335\u001B[39m \u001B[43mtrain_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 315\u001B[39m, in \u001B[36mtrain_detector\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    312\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(test_data), \u001B[38;5;28mlen\u001B[39m(test_data[\u001B[32m0\u001B[39m]), test_data[\u001B[32m1\u001B[39m])\n\u001B[32m    314\u001B[39m \u001B[38;5;66;03m# Train the models and save the results\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m315\u001B[39m \u001B[43mtrain_and_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbuckets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[92]\u001B[39m\u001B[32m, line 222\u001B[39m, in \u001B[36mtrain_and_save\u001B[39m\u001B[34m(models, train_data, test_data, feature_names, class_names, seed, replot_only)\u001B[39m\n\u001B[32m    219\u001B[39m     model_info[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m].set_params(random_state=seed)\n\u001B[32m    221\u001B[39m clf = GridSearchCV(model_info[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], model_info[\u001B[33m\"\u001B[39m\u001B[33mparam_grid\u001B[39m\u001B[33m\"\u001B[39m], cv=\u001B[32m5\u001B[39m, verbose=\u001B[32m10\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[43mclf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    224\u001B[39m y_pred = clf.predict(X_test)\n\u001B[32m    225\u001B[39m y_train_pred = clf.predict(X_train)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ml/lib/python3.12/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ml/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ml/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1569\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1570\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1571\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ml/lib/python3.12/site-packages/sklearn/model_selection/_search.py:982\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m    970\u001B[39m out = parallel(\n\u001B[32m    971\u001B[39m     delayed(_fit_and_score)(\n\u001B[32m    972\u001B[39m         clone(base_estimator),\n\u001B[32m    973\u001B[39m         X,\n\u001B[32m    974\u001B[39m         y,\n\u001B[32m    975\u001B[39m         train=train,\n\u001B[32m    976\u001B[39m         test=test,\n\u001B[32m    977\u001B[39m         parameters=parameters,\n\u001B[32m    978\u001B[39m         split_progress=(split_idx, n_splits),\n\u001B[32m    979\u001B[39m         candidate_progress=(cand_idx, n_candidates),\n\u001B[32m    980\u001B[39m         **fit_and_score_kwargs,\n\u001B[32m    981\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m982\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m \u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m )\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ml/lib/python3.12/site-packages/sklearn/model_selection/_split.py:404\u001B[39m, in \u001B[36m_BaseKFold.split\u001B[39m\u001B[34m(self, X, y, groups)\u001B[39m\n\u001B[32m    402\u001B[39m n_samples = _num_samples(X)\n\u001B[32m    403\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_splits > n_samples:\n\u001B[32m--> \u001B[39m\u001B[32m404\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    405\u001B[39m         (\n\u001B[32m    406\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mCannot have number of splits n_splits=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m greater\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m than the number of samples: n_samples=\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    408\u001B[39m         ).format(\u001B[38;5;28mself\u001B[39m.n_splits, n_samples)\n\u001B[32m    409\u001B[39m     )\n\u001B[32m    411\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().split(X, y, groups):\n\u001B[32m    412\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "\u001B[31mValueError\u001B[39m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "#!rm -r runs\n",
    "if False:\n",
    "  main1()\n",
    "else:\n",
    "  main2()\n",
    "  !tail /content/runs/2025-03-19_17-42-58/log.txt\n",
    "  #!rm -r LLama\n",
    "  from sklearn.metrics import ConfusionMatrixDisplay\n",
    "  models = {\n",
    "          \"LogisticRegression\": {\n",
    "              \"model\": LogisticRegression(max_iter=1000),\n",
    "              \"param_grid\": {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "          },\n",
    "          \"DecisionTree\": {\n",
    "              \"model\": DecisionTreeClassifier(),\n",
    "              \"param_grid\": {\n",
    "                  \"max_depth\": [5, 10, 15, 20],\n",
    "                  \"min_samples_split\": [2, 5, 10],\n",
    "                  \"min_samples_leaf\": [1, 2, 4],\n",
    "              },\n",
    "          }\n",
    "      }\n",
    "  for model_name in models:\n",
    "    with open(f\"{model_name}_confusion_matrix.json\") as f:\n",
    "      confm = json.load(f)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=np.array(confm))\n",
    "    disp.plot()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:33.605695300Z",
     "start_time": "2025-07-30T22:55:30.220992100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFbFwcWQz1zl",
    "ExecuteTime": {
     "end_time": "2025-07-30T22:55:33.605695300Z",
     "start_time": "2025-07-30T22:55:33.605695300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "models = {\n",
    "        \"LogisticRegression\": {\n",
    "            \"model\": LogisticRegression(max_iter=1000),\n",
    "            \"param_grid\": {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model\": DecisionTreeClassifier(),\n",
    "            \"param_grid\": {\n",
    "                \"max_depth\": [None, 5, 10, 15, 20],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "for model_name in models:\n",
    "  with open(f\"{model_name}_confusion_matrix.json\") as f:\n",
    "    confm = json.load(f)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=np.array(confm))\n",
    "  disp.plot()\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GI0LkhiQTdco"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
